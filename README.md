
# ğŸ³ Docker Data Pipeline (PySpark + dbt + Postgres + Streamlit)

This project demonstrates a simple end-to-end data pipeline for Data Engineers using Docker.

## âœ… Stack Overview

| Component     | Description                        |
|---------------|------------------------------------|
| **PostgreSQL**| Stores raw and transformed data    |
| **PySpark**   | Reads & processes input CSV        |
| **dbt**       | Transforms data in the database    |
| **Streamlit** | Visualizes the final data output   |

---

## ğŸ“‹ Prerequisites

Before getting started, make sure you have **Docker Desktop** installed on your system:

- **Docker Desktop**: Download and install from [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)

---

## ğŸš€ Getting Started

### 1. Clone or unzip the project

```bash
cd docker
```

### 2. Build and start the containers

```bash
docker-compose build
docker-compose up -d
```

---

## ğŸ§ª Run Data Pipeline Steps

### 1. Run PySpark Job (loads CSV into PostgreSQL)

```bash
docker exec -it spark spark-submit /opt/spark/spark_job.py
```

### 1.5. (Optional) Connect to PostgreSQL and Check Tables

You can directly connect to the PostgreSQL database to verify the data:

```bash
docker exec -it pg psql -U demo -d visits_db
```

Once connected, you can run these SQL queries:

View all tables \dt
View columns of a table \d raw_visits or \d user_sessions
View first rows SELECT * FROM raw_visits LIMIT 5;
Exit PostgreSQL \q


-- Exit PostgreSQL
\q
```

**Connection Details:**
- Host: `localhost` (or `pg` from inside containers)
- Port: `5432`
- Database: `visits_db`
- Username: `demo`
- Password: `demo`

---

### 2. Run dbt Transformation

```bash
docker exec -it dbt bash
cd /usr/app
dbt run --profiles-dir profiles
exit
```

### 3. Open the Streamlit Dashboard

Visit ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.csv                 # Input data file
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ user_sessions.sql     # dbt model
â”‚   â”œâ”€â”€ profiles/
â”‚   â”‚   â””â”€â”€ profiles.yml          # dbt connection config
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ custom_spark/
â”‚   â”œâ”€â”€ Dockerfile                # Custom Spark image
â”‚   â””â”€â”€ postgresql-42.2.5.jar     # JDBC Driver (add manually)
â”œâ”€â”€ spark_job.py                  # PySpark transformation script
â”œâ”€â”€ streamlit_app.py              # Streamlit dashboard
â””â”€â”€ docker-compose.yml
```

---

## âœ¨ What You Can Show

- How Spark processes data from CSV and loads it into a warehouse
- How dbt runs transformations in a reproducible way
- How Streamlit creates real-time, interactive dashboards
- How Docker makes the entire pipeline portable and consistent

---

Happy presenting! ğŸš€
