
# ğŸ³ Docker Data Pipeline (PySpark + dbt + Postgres + Streamlit)

This project demonstrates a simple end-to-end data pipeline for Data Engineers using Docker.

## âœ… Stack Overview

| Component     | Description                        |
|---------------|------------------------------------|
| **PostgreSQL**| Stores raw and transformed data    |
| **PySpark**   | Reads & processes input CSV        |
| **dbt**       | Transforms data in the database    |
| **Streamlit** | Visualizes the final data output   |

---

## ğŸ“‹ Prerequisites

Before getting started, make sure you have **Docker Desktop** installed on your system:

- **Docker Desktop**: Download and install from [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)

---

## ğŸš€ Getting Started

### 1. Clone or unzip the project

```bash
cd docker
```

### 2. Build and start the containers

```bash
docker-compose build
docker-compose up -d
```

---

## ğŸ§ª Run Data Pipeline Steps

### 1. Run PySpark Job (loads CSV into PostgreSQL)

```bash
docker exec -it spark spark-submit /opt/spark/spark_job.py
```

### 2. Run dbt Transformation

```bash
docker exec -it dbt bash
cd /usr/app
dbt run --profiles-dir profiles
exit
```

### 3. Open the Streamlit Dashboard

Visit ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.csv                 # Input data file
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ user_sessions.sql     # dbt model
â”‚   â”œâ”€â”€ profiles/
â”‚   â”‚   â””â”€â”€ profiles.yml          # dbt connection config
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ custom_spark/
â”‚   â”œâ”€â”€ Dockerfile                # Custom Spark image
â”‚   â””â”€â”€ postgresql-42.2.5.jar     # JDBC Driver (add manually)
â”œâ”€â”€ spark_job.py                  # PySpark transformation script
â”œâ”€â”€ streamlit_app.py              # Streamlit dashboard
â””â”€â”€ docker-compose.yml
```

---

## âœ¨ What You Can Show

- How Spark processes data from CSV and loads it into a warehouse
- How dbt runs transformations in a reproducible way
- How Streamlit creates real-time, interactive dashboards
- How Docker makes the entire pipeline portable and consistent

---

Happy presenting! ğŸš€
